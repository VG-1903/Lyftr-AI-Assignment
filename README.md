# **ğŸ” Lyftr AI Assignment â€“ Intelligent Web Scraper**

A sophisticated **full-stack intelligent web scraper** featuring hybrid scraping, structured content extraction, dynamic fallback logic, and a React dashboard.  
Built using the **MERN stack (MongoDB, Express, React, Node.js)**.

---

## **âœ¨ Features**

### **ğŸ¤– Intelligent Scraping Engine**
- **Static Scraping (Cheerio)** for fast HTML parsing  
- **Dynamic Scraping (Puppeteer)** for JavaScript-rendered content  
- **Automatic Fallback Logic** (static â†’ dynamic if content is insufficient)  
- **Structured Extraction** of:
  - Headings  
  - Paragraphs  
  - Links  
  - Images  
  - Lists  
  - Tables  

### **ğŸ¯ Smart Enhancements**
- **Automatic Port Detection** (3000/5000 â†’ next free port)  
- **Noise Filtering** (ads, banners, boilerplate content)  
- **Overlay Removal** (modals, cookie banners)  
- **Auto-Scrolling** for lazy-loaded websites  
- **Click Automation** for â€œLoad Moreâ€ / â€œShow Moreâ€ buttons  

### **ğŸ“± Integrations**
- **MongoDB Atlas** for storing scraped results  
- **Twilio WhatsApp API** for sending scraped data  
- **REST API** for external automation  
- **React Dashboard UI** for visualization  

---

## **ğŸš€ Quick Start**

### **Prerequisites**
- **Node.js 16+**  
- **npm**  
- **Git**  
- **MongoDB Atlas account**

---

## **ğŸ“¦ Installation**

```bash
# Clone the repository
git clone https://github.com/VG-1903/Lyftr-AI-Assignment.git
cd Lyftr-AI-Assignment

# Install dependencies automatically
# Windows:
install.bat

# macOS/Linux:
chmod +x install.sh
./install.sh




## **ğŸŒ Access the Application**

Frontend: http://localhost:3000

Backend API: http://localhost:5000

Health Check: http://localhost:5000/healthz

If ports are busy, the launcher scripts automatically pick available ports in the ranges:

Frontend: 3000â€“3010

Backend: 5000â€“5010

---

## **ğŸ—ï¸ Architecture**
React Frontend â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Express Backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Scraper Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Target Website
        â–²                          â”‚
        â”‚                          â–¼
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MongoDB Atlas

---

## **ğŸ”„ Scraping Strategy**
Start Scrape
   â”‚
   â–¼
Static Scrape (Cheerio)
   â”‚
   â”œâ”€â”€ If extracted content > 300 chars â†’ Return result
   â”‚
   â–¼
Fallback to Puppeteer
   â”‚
Dynamic Scraping (JS Execution + Interactions)
   â–¼
Return Final Results

---

## **ğŸ§© Project Structure**
Lyftr-AI-Assignment/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.js
â”‚   â”œâ”€â”€ scraper.js
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ run.bat
â”œâ”€â”€ run.sh
â”œâ”€â”€ install.bat
â”œâ”€â”€ install.sh
â”œâ”€â”€ design_notes.md
â”œâ”€â”€ setup-guide.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README-PORTS.md
â””â”€â”€ README.md



## **ğŸ§ª Testing**
### **Backend tests**
cd backend
npm test

### **Frontend tests**
cd frontend
npm test

### **Test scraping**
curl -X POST http://localhost:5000/api/scrape -d '{"url": "https://example.com"}'

---

## **ğŸš¢ Deployment (Optional)**
Deploy to Vercel
### Backend
cd backend
vercel --prod

### Frontend
cd frontend
vercel --prod

---

## **Environment Variables**
### **Backend**
MONGO_URI=your_mongodb_uri
NODE_ENV=production
TWILIO_ACCOUNT_SID=your_sid
TWILIO_AUTH_TOKEN=your_token

### **Frontend**
REACT_APP_API_URL=https://your-backend.vercel.app

##**ğŸ Troubleshooting**
Issue	Solution
Ports busy	Use kill-ports.bat or kill-ports.sh
MongoDB fails	Check .env + IP whitelist
Puppeteer errors	Install missing OS dependencies
React not loading	Ensure backend is running

.---

###**â¤ï¸ Built by Vansh Gaba**

